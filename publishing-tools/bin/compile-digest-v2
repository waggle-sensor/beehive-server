#!/usr/bin/env python3
import argparse
import hashlib
import datetime
import os
import shutil
import gzip
import multiprocessing
import time
import publishing
import sys
from jinja2 import Template
import csv
from io import StringIO
import logging
from contextlib import contextmanager
from collections import defaultdict


@contextmanager
def timed(*args):
    context_start_time = time.time()
    yield
    context_end_time = time.time()
    print(*args, context_end_time - context_start_time)


def read_file(filename):
    with open(filename, 'rb') as file:
        return file.read()


def read_compressed_file(filename):
    return gzip.decompress(read_file(filename))


def ensure_dir(filename):
    os.makedirs(os.path.dirname(filename), exist_ok=True)


def hash_dependencies(filenames):
    hash = hashlib.sha1()

    for filename in sorted(filenames):
        with open(filename, 'rb') as file:
            hash.update(file.read())

    return hash.digest()


# rename checkpoint -> digest
def get_checkpoint(target):
    try:
        with open(target + '.digest', 'rb') as file:
            return file.read()
    except FileNotFoundError:
        return bytes()


def set_checkpoint(target, hash):
    ensure_dir(target)

    with open(target + '.digest', 'wb') as file:
        file.write(hash)


# NOTE Safer to just do simple listdir rather than walk?
def find_data_files(top, rel=''):
    for entry in os.scandir(top):
        if entry.is_dir():
            yield from find_data_files(entry, os.path.join(rel, entry.name))
        if entry.is_file() and entry.name.endswith('.csv.gz'):
            yield os.path.join(rel, entry.name)


# NOTE Simpler to just enumerate nodes and then check dates?
def find_data_files_for_project(data_dir, project_metadata):
    nodes_by_id = {node['node_id']: node for node in project_metadata}

    def isvalid(file):
        node_id, date = file.rstrip('.csv.gz').split('/')
        date = datetime.datetime.strptime(date, '%Y-%m-%d')
        return (node_id in nodes_by_id and
                any(date in interval for interval in nodes_by_id[node_id]['commissioned']))

    return filter(isvalid, find_data_files(data_dir))


def update_filtered_files(data_dirs, build_dir, project_dir, **kwargs):
    processes = kwargs.get('processes', None)
    complete = kwargs.get('complete', False)

    project_metadata = publishing.load_project_metadata(project_dir)
    sensor_metadata = publishing.load_sensor_metadata(os.path.join(project_dir, 'sensors.csv'))

    if complete:
        configs = []
    else:
        configs = [os.path.join(project_dir, 'sensors.csv')]

    dependencies_by_target = defaultdict(list)

    for data_dir in data_dirs:
        for file in find_data_files_for_project(data_dir, project_metadata):
            target = os.path.join(build_dir, file)
            source = os.path.join(data_dir, file)
            dependencies_by_target[target].append(source)

    tasks = []

    for target, dependencies in dependencies_by_target.items():
        tasks.append((target, dependencies, configs))

    with multiprocessing.Pool(processes) as pool:
        pool.starmap(update_filtered_file_if_needed, tasks)


def update_filtered_file_if_needed(target, dependencies, configs):
    hash = hash_dependencies(dependencies + configs)

    if get_checkpoint(target) == hash:
        return

    print('make', target)
    start = time.time()

    try:
        update_filtered_file(target, dependencies, configs)
        print('done', target, time.time() - start)
        set_checkpoint(target, hash)
    except Exception:
        print('fail', target)
        logging.exception('filter failed')


def update_filtered_file(target, dependencies, configs):
    ensure_dir(target)

    all_data = []

    if configs:
        # TODO Fix this...it's very unclear what configs[0] is.
        metadata = publishing.load_sensor_metadata(configs[0])

    for source in dependencies:
        data = read_compressed_file(source)

        if configs:
            data = apply_sensor_filter(metadata, data)

        rows = data.splitlines()

        if rows[0].startswith(b'timestamp'):
            rows = rows[1:]

        rows.sort()
        rows.append(b'')

        data = gzip.compress(b'\n'.join(rows))
        all_data.append(data)

    with open(target, 'wb') as outfile:
        for data in all_data:
            outfile.write(data)


def apply_sensor_filter(metadata, data):
    reader = StringIO(data.decode())
    writer = StringIO()
    publishing.filter_sensors(metadata, reader, writer)
    return writer.getvalue().encode()


def update_date_files(data_dir, build_dir, project_dir, **kwargs):
    processes = kwargs.get('processes', None)

    project_metadata = publishing.load_project_metadata(project_dir)

    configs = []

    date_dependencies = {}

    for file in find_data_files_for_project(data_dir, project_metadata):
        target = os.path.join(build_dir, os.path.basename(file))

        if target not in date_dependencies:
            date_dependencies[target] = []

        date_dependencies[target].append(os.path.join(data_dir, file))

    tasks = []

    for target, dependencies in sorted(date_dependencies.items(), reverse=True):
        tasks.append((target, dependencies, configs))

    # NOTE May want to adapt based on available memory, not just processors.
    with multiprocessing.Pool(processes) as pool:
        pool.starmap(update_date_file_if_needed, tasks)


def update_date_file_if_needed(target, dependencies, configs):
    hash = hash_dependencies(dependencies + configs)

    if get_checkpoint(target) == hash:
        return

    print('make', target)
    start = time.time()
    update_date_file(target, dependencies)
    print('done', target, time.time() - start)
    set_checkpoint(target, hash)


def update_date_file(target, dependencies):
    rows = []

    for source in dependencies:
        rows.extend(read_compressed_file(source).splitlines())

    rows.sort()

    # ensure blank line at end
    rows.append(b'')
    data = gzip.compress(b'\n'.join(rows))

    ensure_dir(target)

    with open(target, 'wb') as file:
        file.write(data)


# NOTE faster to cat rather than check if needed
def update_combined_file(data_dir, build_dir):
    target = os.path.join(build_dir, 'data.csv.gz')
    dependencies = [os.path.join(data_dir, file) for file in sorted(find_data_files(data_dir))]

    print('make', target)
    start = time.time()

    offsets = []

    with open(target, 'wb') as file:
        # restore header to final result
        header = b'timestamp,node_id,subsystem,sensor,parameter,value_raw,value_hrf\n'
        file.write(gzip.compress(header))

        for dep in dependencies:
            offset = file.tell()
            size = file.write(read_file(dep))
            offsets.append((os.path.basename(dep).rstrip('.csv.gz'), offset, size))

    with open(os.path.join(build_dir, 'offsets.csv'), 'w') as file:
        writer = csv.writer(file)
        writer.writerow(['date', 'offset', 'size'])
        writer.writerows(offsets)

    print('done', target, time.time() - start)


def move_file(src, dst):
    ensure_dir(dst)
    shutil.move(src, dst)


def copy_file(src, dst):
    ensure_dir(dst)
    shutil.copy(src, dst)


def copy_file_if_exists(src, dst):
    try:
        copy_file(src, dst)
    except FileNotFoundError:
        pass


def read_file_or_empty(filename):
    try:
        return read_file(filename).decode()
    except FileNotFoundError:
        return ''


def render_template(filename, template, *args, **kwargs):
    output = Template(read_file(template).decode()).render(*args, **kwargs)

    with open(filename, 'w') as file:
        file.write(output)


# maybe just use a single, general render template function
def update_project_files(build_dir, project_dir, complete=False, cleanup=True):
    print('make archive')

    start = time.time()

    project_id = os.path.basename(project_dir)
    date = datetime.datetime.utcnow().strftime('%Y-%m-%d')

    digest_dir = os.path.join(build_dir,
                              '{}.latest'.format(project_id),
                              '{}.{}'.format(project_id, date))

    copy_file(os.path.join(project_dir, 'nodes.csv'),
              os.path.join(digest_dir, 'nodes.csv'))

    copy_file(os.path.join(project_dir, 'sensors.csv'),
              os.path.join(digest_dir, 'sensors.csv'))

    copy_file_if_exists(os.path.join(project_dir, 'DUA.txt'),
                        os.path.join(digest_dir, 'DUA.txt'))

    if cleanup:
        move_file(os.path.join(build_dir, 'data.csv.gz'),
                  os.path.join(digest_dir, 'data.csv.gz'))
    else:
        copy_file(os.path.join(build_dir, 'data.csv.gz'),
                  os.path.join(digest_dir, 'data.csv.gz'))

    move_file(os.path.join(build_dir, 'offsets.csv'),
              os.path.join(digest_dir, 'offsets.csv'))

    template_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0]))), 'docs')

    render_template(os.path.join(digest_dir, 'README.md'),
                    os.path.join(template_dir, 'digest-readme.md'),
                    header=read_file_or_empty(os.path.join(project_dir, 'header.md')),
                    complete=complete)

    now = datetime.datetime.now()

    nodes = publishing.load_project_metadata(project_dir)

    intervals = [interval for node in nodes for interval in node['commissioned']]

    try:
        data_start_date = min(interval.start for interval in intervals)
    except ValueError:
        data_start_date = now

    try:
        data_end_date = max(interval.end or now for interval in intervals)
    except ValueError:
        data_end_date = now

    with open(os.path.join(digest_dir, 'provenance.csv'), 'w') as outfile:
        writer = csv.writer(outfile)

        writer.writerow([
            'data_format_version',
            'project_id',
            'data_start_date',
            'data_end_date',
            'creation_date',
            'url',
        ])

        writer.writerow([
            '2',
            project_id,
            data_start_date.strftime('%Y/%m/%d %H:%M:%S'),
            data_end_date.strftime('%Y/%m/%d %H:%M:%S'),
            now.strftime('%Y/%m/%d %H:%M:%S'),
            'http://www.mcs.anl.gov/research/projects/waggle/downloads/datasets/{}.latest.tar'.format(project_id),
        ])

    shutil.make_archive(base_name=os.path.dirname(digest_dir),
                        root_dir=os.path.dirname(digest_dir),
                        base_dir=os.path.basename(digest_dir),
                        format='tar')

    # clean up archive dir
    if cleanup:
        shutil.rmtree(os.path.dirname(digest_dir))

    print('done archive', time.time() - start)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-p', '--processes', type=int, default=None, help='Number of worker processes.')
    parser.add_argument('--no-cleanup', action='store_false', dest='cleanup', help='Leave non-cached byproducts.')
    parser.add_argument('--complete', action='store_true', help='Compile as complete digest.')
    parser.add_argument('-d', '--data', action='append', help='Include data tree in build.')
    parser.add_argument('build_dir', help='Directory used for building digest.')
    parser.add_argument('project_dir', help='Directory containing project metadata.')
    args = parser.parse_args()

    data_dirs = sorted({os.path.abspath(p) for p in args.data})
    build_dir = os.path.abspath(args.build_dir)
    project_dir = os.path.abspath(args.project_dir)
    filtered_dir = os.path.join(build_dir, 'filtered')
    dates_dir = os.path.join(build_dir, 'dates')

    project_id = os.path.basename(project_dir)

    os.makedirs(build_dir, exist_ok=True)
    os.makedirs(filtered_dir, exist_ok=True)
    os.makedirs(dates_dir, exist_ok=True)

    with timed('compile_digest', project_id):
        with timed('update_filtered_files', project_id):
            update_filtered_files(data_dirs, filtered_dir, project_dir, processes=args.processes, complete=args.complete)
        with timed('update_date_files', project_id):
            update_date_files(filtered_dir, dates_dir, project_dir, processes=args.processes)
        with timed('update_combined_file', project_id):
            update_combined_file(dates_dir, build_dir)
        with timed('update_project_files', project_id):
            update_project_files(build_dir, project_dir, complete=args.complete, cleanup=args.cleanup)
