#!/usr/bin/env python3
import sys
import os
import subprocess
import argparse
import shutil
import publishing
import gzip
import csv
from datetime import datetime
from jinja2 import Template

parser = argparse.ArgumentParser()
parser.add_argument('project_ids', nargs='*', help='Project IDs to compile digest for.')
args = parser.parse_args()

# resolve environment paths
program = os.path.abspath(sys.argv[0])
program_dir = os.path.dirname(os.path.dirname(program))

try:
    datasets_dir = os.environ['DATASETS_DIR']
except KeyError:
    print('Please set the environmental variable DATASETS_DIR to a datasets base directory.')
    sys.exit(1)

try:
    projects_dir = os.environ['PROJECTS_DIR']
except KeyError:
    print('Please set the environmental variable PROJECTS_DIR to a projects base directory.')
    sys.exit(1)

try:
    digests_dir = os.environ['DIGESTS_DIR']
except KeyError:
    print('Please set the environmental variable DIGESTS_DIR to a digests build directory.')
    sys.exit(1)


def ensure_exists(dir):
    os.mkdirs(dir, exist_ok=True)


def needs_refresh(pair):
    src, dst = pair

    try:
        src_mtime = os.path.getmtime(src)
    except FileNotFoundError:
        return False

    try:
        dst_mtime = os.path.getmtime(dst)
    except FileNotFoundError:
        return True

    return dst_mtime < src_mtime


def make_project_digest(project_id):
    print('[{}] Preparing build tree.'.format(project_id))

    project_dir = os.path.join(projects_dir, project_id)
    ensure_exists(program_dir)

    build_dir = os.path.join(digests_dir, project_id)
    ensure_exists(build_dir)

    digest_dir = os.path.join(build_dir, '{}.latest'.format(project_id), '{}.{}'.format(project_id, datetime.utcnow().strftime('%Y-%m-%d')))
    ensure_exists(digest_dir)

    staging_dir = os.path.join(build_dir, 'staging')
    ensure_exists(staging_dir)

    print('[{}] Copy metadata.'.format(project_id))

    with open(os.path.join(program_dir, 'docs', 'digest-readme.md')) as f:
        template = Template(f.read())

    try:
        with open(os.path.join(project_dir, 'header.md')) as f:
            header = f.read()
    except FileNotFoundError:
        header = ''

    with open(os.path.join(digest_dir, 'README.md'), 'w') as f:
        f.write(template.render(header=header, project_id=project_id))

    try:
        shutil.copy(os.path.join(project_dir, 'DUA.txt'), os.path.join(digest_dir, 'DUA.txt'))
    except Exception:
        pass

    shutil.copy(os.path.join(project_dir, 'nodes.csv'), os.path.join(digest_dir, 'nodes.csv'))
    shutil.copy(os.path.join(project_dir, 'sensors.csv'), os.path.join(digest_dir, 'sensors.csv'))

    print('[{}] Staging data files.'.format(project_id))

    nodes = publishing.load_project_metadata(project_dir)

    # sort candinate files in latest first order
    candidates = sorted(publishing.published_dates(nodes),
                        key=lambda item: item[1], reverse=True)

    file_pairs = []

    for node, date in candidates:
        src = os.path.join(datasets_dir, node['node_id'], date.strftime('%Y-%m-%d.csv.gz'))

        os.makedirs(os.path.join(staging_dir, node['node_id']), exist_ok=True)
        dst = os.path.join(staging_dir, node['node_id'], date.strftime('%Y-%m-%d.csv.gz'))
        file_pairs.append((src, dst))

    for src, dst in filter(needs_refresh, file_pairs):
        try:
            sensors_metadata = os.path.join(project_dir, 'sensors.csv')
            project_metadata = os.path.join(project_dir)

            script_template = '''
            gzip -dc '{src}' |
            {program_dir}/bin/filter-sensors '{sensors_metadata}' |
            {program_dir}/bin/filter-view '{project_metadata}' |
            gzip > '{dst}.tmp'
            '''

            script = script_template.format(src=src,
                                            dst=dst,
                                            program_dir=program_dir,
                                            project_dir=project_dir,
                                            sensors_metadata=sensors_metadata,
                                            project_metadata=project_metadata)

            subprocess.check_output(script, shell=True)

            os.rename(dst + '.tmp', dst)

            print('[{}] Prepared file {}.'.format(project_id, src))
        except Exception as exc:
            print('[{}] Failed to prepare file {}!'.format(project_id, src))

    print('[{}] Compiling staged files.'.format(project_id))

    with open(os.path.join(digest_dir, 'data.csv'), 'wb') as outfile:
        outfile.write(b'node_id,timestamp,plugin,sensor,parameter,value\n')

        for root, _, filenames in os.walk(staging_dir):
            for filename in filenames:
                if not filename.endswith('.csv.gz'):
                    continue
                with open(os.path.join(root, filename), 'rb') as infile:
                    data = gzip.decompress(infile.read())
                for line in data.splitlines():
                    fields = line.split(b';')
                    outfile.write(fields[0])
                    outfile.write(b',')
                    outfile.write(fields[1])
                    outfile.write(b',')
                    outfile.write(fields[2])
                    outfile.write(b',')
                    outfile.write(fields[4])
                    outfile.write(b',')
                    outfile.write(fields[5])
                    outfile.write(b',')
                    outfile.write(fields[6])
                    outfile.write(b'\n')

    print('[{}] Compiling archive.'.format(project_id))

    now = datetime.now()

    intervals = [interval for node in nodes for interval in node['commissioned']]

    try:
        data_start_date = min(interval.start for interval in intervals)
    except ValueError:
        data_start_date = now

    try:
        data_end_date = max(interval.end or now for interval in intervals)
    except ValueError:
        data_end_date = now

    with open(os.path.join(digest_dir, 'provenance.csv'), 'w') as outfile:
        writer = csv.writer(outfile)

        writer.writerow([
            'data_format_version',
            'project_id',
            'data_start_date',
            'data_end_date',
            'creation_date',
            'url',
        ])

        writer.writerow([
            '1',
            project_id,
            data_start_date.strftime('%Y/%m/%d %H:%M:%S'),
            data_end_date.strftime('%Y/%m/%d %H:%M:%S'),
            now.strftime('%Y/%m/%d %H:%M:%S'),
            'http://www.mcs.anl.gov/research/projects/waggle/downloads/datasets/{}.latest.tar.gz'.format(project_id),
        ])

    shutil.make_archive(base_name=os.path.join(build_dir, '{}.latest'.format(project_id)),
                        format='gztar',
                        root_dir=os.path.dirname(digest_dir),
                        base_dir=os.path.basename(digest_dir))


if __name__ == '__main__':
    for project_id in args.project_ids:
        make_project_digest(project_id)
